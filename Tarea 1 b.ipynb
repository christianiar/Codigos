{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from matplotlib.pylab import hist, show\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import keras as keras\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoje = time.strftime(\"Z %d-%m terceiro do Dia\")\n",
    "my_path = 'Imagens2'\n",
    "sgd = SGD(lr=0.01)\n",
    "\n",
    "datos= pd.read_csv(\"roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          \n",
    "df_test=datos[int(0.85*total)::]\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = X_train_scaled.pop('Eat').values.reshape(-1,1)\n",
    "\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "y_val_scaled = X_val_scaled.pop('Eat').values.reshape(-1,1)\n",
    "\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "y_test_scaled = X_test_scaled.pop('Eat').values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 1s 134us/step - loss: 0.1095 - val_loss: 0.0956\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0645 - val_loss: 0.0500\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0535 - val_loss: 0.0394\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0478 - val_loss: 0.0397\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 1s 119us/step - loss: 0.0453 - val_loss: 0.0369\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 1s 119us/step - loss: 0.0421 - val_loss: 0.0332\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0408 - val_loss: 0.0393\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0384 - val_loss: 0.0372\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0372 - val_loss: 0.0306\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0354 - val_loss: 0.0316\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 1s 129us/step - loss: 0.0333 - val_loss: 0.0317\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 1s 134us/step - loss: 0.0327 - val_loss: 0.0269\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0313 - val_loss: 0.0268\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0301 - val_loss: 0.0249\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0291 - val_loss: 0.0262\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0278 - val_loss: 0.0241\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0270 - val_loss: 0.0256\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0257 - val_loss: 0.0223\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0247 - val_loss: 0.0207\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0237 - val_loss: 0.0196\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0223 - val_loss: 0.0191\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0180\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0205 - val_loss: 0.0200\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0197 - val_loss: 0.0184\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0187 - val_loss: 0.0207\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0176 - val_loss: 0.0181\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0171 - val_loss: 0.0233\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0162 - val_loss: 0.0148\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0158 - val_loss: 0.0167\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0152 - val_loss: 0.0171\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 1s 129us/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0135 - val_loss: 0.0124\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0128 - val_loss: 0.0162\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0128 - val_loss: 0.0124\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0119 - val_loss: 0.0134\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0115 - val_loss: 0.0128\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0102 - val_loss: 0.0098\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0099 - val_loss: 0.0102\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0093 - val_loss: 0.0092\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0081 - val_loss: 0.0088\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0080 - val_loss: 0.0084\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0077 - val_loss: 0.0083\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0077 - val_loss: 0.0080\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0076 - val_loss: 0.0081\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0072 - val_loss: 0.0077\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0072 - val_loss: 0.0098\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0069 - val_loss: 0.0077\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0068 - val_loss: 0.0074\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0067 - val_loss: 0.0150\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0061 - val_loss: 0.0072\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0062 - val_loss: 0.0069\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0059 - val_loss: 0.0064\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0059 - val_loss: 0.0071\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0058 - val_loss: 0.0075\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0057 - val_loss: 0.0071\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0055 - val_loss: 0.0063\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0055 - val_loss: 0.0060\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0054 - val_loss: 0.0061\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0053 - val_loss: 0.0065\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 1s 115us/step - loss: 0.0052 - val_loss: 0.0068\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 1s 108us/step - loss: 0.0053 - val_loss: 0.0060\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 1s 131us/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0051 - val_loss: 0.0060\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0050 - val_loss: 0.0058\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0050 - val_loss: 0.0055\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0049 - val_loss: 0.0055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0049 - val_loss: 0.0055\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0048 - val_loss: 0.0075\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0047 - val_loss: 0.0135\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0045 - val_loss: 0.0078\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0046 - val_loss: 0.0064\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0044 - val_loss: 0.0068\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0044 - val_loss: 0.0055\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0043 - val_loss: 0.0065\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0042 - val_loss: 0.0051\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0043 - val_loss: 0.0061\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0041 - val_loss: 0.0050\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0042 - val_loss: 0.0049\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0041 - val_loss: 0.0066\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0041 - val_loss: 0.0064\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0040 - val_loss: 0.0064\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0040 - val_loss: 0.0049\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0039 - val_loss: 0.0053\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0039 - val_loss: 0.0072\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0039 - val_loss: 0.0063\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0038 - val_loss: 0.0051\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0039 - val_loss: 0.0049\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0038 - val_loss: 0.0056\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0039 - val_loss: 0.0046\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0037 - val_loss: 0.0046\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0037 - val_loss: 0.0049\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0037 - val_loss: 0.0062\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0035 - val_loss: 0.0057\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0035 - val_loss: 0.0055\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0034 - val_loss: 0.0045\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0035 - val_loss: 0.0062\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0034 - val_loss: 0.0047\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0033 - val_loss: 0.0044\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0033 - val_loss: 0.0049\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0032 - val_loss: 0.0050\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0033 - val_loss: 0.0048\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0032 - val_loss: 0.0041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0031 - val_loss: 0.0045\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0031 - val_loss: 0.0044\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0031 - val_loss: 0.0045\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 1s 114us/step - loss: 0.0030 - val_loss: 0.0046\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 1s 107us/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 1s 129us/step - loss: 0.0030 - val_loss: 0.0043\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0030 - val_loss: 0.0051\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0029 - val_loss: 0.0044\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0029 - val_loss: 0.0052\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0029 - val_loss: 0.0044\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0029 - val_loss: 0.0075\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0027 - val_loss: 0.0036\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0027 - val_loss: 0.0036\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0027 - val_loss: 0.0036\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0026 - val_loss: 0.0036\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 1s 137us/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 1s 134us/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0027 - val_loss: 0.0036\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0026 - val_loss: 0.0035\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0026 - val_loss: 0.0036\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0026 - val_loss: 0.0037\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0027 - val_loss: 0.0043\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0026 - val_loss: 0.0034\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0026 - val_loss: 0.0035\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0026 - val_loss: 0.0037\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0026 - val_loss: 0.0036\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0026 - val_loss: 0.0035\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0026 - val_loss: 0.0035\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0026 - val_loss: 0.0034\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0026 - val_loss: 0.0036\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0026 - val_loss: 0.0034\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0025 - val_loss: 0.0035\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0025 - val_loss: 0.0036\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0025 - val_loss: 0.0037\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0025 - val_loss: 0.0037\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0025 - val_loss: 0.0038\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 1s 135us/step - loss: 0.0025 - val_loss: 0.0035\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 1s 130us/step - loss: 0.0025 - val_loss: 0.0043\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0025 - val_loss: 0.0045\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 1s 131us/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0025 - val_loss: 0.0035\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0025 - val_loss: 0.0035\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0026 - val_loss: 0.0039\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0025 - val_loss: 0.0037\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0025 - val_loss: 0.0036\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0025 - val_loss: 0.0038\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0025 - val_loss: 0.0039\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0025 - val_loss: 0.0036\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_Trei_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_no_Trei_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 1.0475 - val_loss: 1.2225\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0573 - val_loss: 1.2641\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0480 - val_loss: 1.0291\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0455 - val_loss: 1.0430\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0379 - val_loss: 1.0228\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0429 - val_loss: 1.0104\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0408 - val_loss: 1.0106\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0317 - val_loss: 1.1493\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0328 - val_loss: 1.0219\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0328 - val_loss: 1.0637\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0230 - val_loss: 1.0307\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0245 - val_loss: 1.4021\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0245 - val_loss: 1.0280\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0223 - val_loss: 1.3217\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0219 - val_loss: 1.0106\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0189 - val_loss: 1.0457\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0270 - val_loss: 1.0706\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0186 - val_loss: 1.2364\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0168 - val_loss: 1.1682\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0184 - val_loss: 1.1183\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 1.0189 - val_loss: 1.0575\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0227 - val_loss: 1.0290\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 1.0200 - val_loss: 1.5676\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0175 - val_loss: 1.0553\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0147 - val_loss: 1.0901\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 1.0124 - val_loss: 1.2818\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 1.0148 - val_loss: 1.0408\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0143 - val_loss: 1.2013\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0155 - val_loss: 1.1275\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0142 - val_loss: 1.0195\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0141 - val_loss: 1.0708\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0142 - val_loss: 1.2292\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0140 - val_loss: 1.1537\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0118 - val_loss: 1.0139\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0103 - val_loss: 1.0267\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0125 - val_loss: 1.1028\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0078 - val_loss: 1.0254\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 1.0087 - val_loss: 1.0653\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0096 - val_loss: 1.0434\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0090 - val_loss: 1.0883\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0068 - val_loss: 1.0393\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0104 - val_loss: 1.0803\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0080 - val_loss: 1.1274\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0087 - val_loss: 1.0864\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0097 - val_loss: 1.0666\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 1.0096 - val_loss: 1.1436\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 1.0073 - val_loss: 1.0591\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0103 - val_loss: 1.0364\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 1.0089 - val_loss: 1.1649\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0091 - val_loss: 1.0619\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 1.0077 - val_loss: 1.0892\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0093 - val_loss: 1.1301\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 1.0089 - val_loss: 1.0533\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0044 - val_loss: 1.0728\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0083 - val_loss: 1.0163\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0079 - val_loss: 1.0752\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0091 - val_loss: 1.1471\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 1.0042 - val_loss: 1.0574\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0067 - val_loss: 1.0315\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0072 - val_loss: 1.0911\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0061 - val_loss: 1.0133\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0069 - val_loss: 1.1581\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0061 - val_loss: 1.0294\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0063 - val_loss: 1.1549\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0052 - val_loss: 1.2226\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0058 - val_loss: 1.0548\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0039 - val_loss: 1.0188\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0067 - val_loss: 1.0212\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0048 - val_loss: 1.0447\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0061 - val_loss: 1.0450\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0046 - val_loss: 1.0314\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0036 - val_loss: 1.1318\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0035 - val_loss: 1.0975\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0058 - val_loss: 1.0215\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0056 - val_loss: 1.0182\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0063 - val_loss: 1.0661\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0048 - val_loss: 1.0584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0044 - val_loss: 1.0464\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0052 - val_loss: 1.0466\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0060 - val_loss: 1.0847\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0040 - val_loss: 1.0626\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0046 - val_loss: 1.0925\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0045 - val_loss: 1.1061\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0050 - val_loss: 1.0215\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 1.0056 - val_loss: 1.0404\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0052 - val_loss: 1.0327\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0034 - val_loss: 1.0693\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0041 - val_loss: 1.0685\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0040 - val_loss: 1.0174\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0031 - val_loss: 1.1100\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0044 - val_loss: 1.0352\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 1.0041 - val_loss: 1.0425\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0023 - val_loss: 1.0606\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0038 - val_loss: 1.0876\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0039 - val_loss: 1.0333\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0047 - val_loss: 1.0921\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0037 - val_loss: 1.0472\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 1.0042 - val_loss: 1.0262\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0044 - val_loss: 1.0907\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0057 - val_loss: 1.0270\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0038 - val_loss: 1.1275\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0023 - val_loss: 1.0859\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0013 - val_loss: 1.0717\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0036 - val_loss: 1.0586\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0037 - val_loss: 1.0240\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0031 - val_loss: 1.0553\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0032 - val_loss: 1.0657\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0027 - val_loss: 1.0716\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0030 - val_loss: 1.0936\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0041 - val_loss: 1.1371\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0031 - val_loss: 1.0953\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0033 - val_loss: 1.0744\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0036 - val_loss: 1.0603\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0037 - val_loss: 1.1114\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 1.0027 - val_loss: 1.0460\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0027 - val_loss: 1.0905\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0006 - val_loss: 1.0112\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0028 - val_loss: 1.0814\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0026 - val_loss: 1.0424\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0017 - val_loss: 1.0171\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0018 - val_loss: 1.0462\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0033 - val_loss: 1.0427\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 1.0034 - val_loss: 1.0579\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0027 - val_loss: 1.0285\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0025 - val_loss: 1.0402\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0014 - val_loss: 1.0410\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0015 - val_loss: 1.0994\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0022 - val_loss: 1.0562\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0017 - val_loss: 1.1275\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0022 - val_loss: 1.0941\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0017 - val_loss: 1.0417\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0022 - val_loss: 1.0701\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0014 - val_loss: 1.0732\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0004 - val_loss: 1.1041\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0023 - val_loss: 1.0362\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0024 - val_loss: 1.0426\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0005 - val_loss: 1.0387\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0010 - val_loss: 1.0313\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0008 - val_loss: 1.0882\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0012 - val_loss: 1.0666\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0014 - val_loss: 1.0242\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0003 - val_loss: 1.0809\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0011 - val_loss: 1.0519\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0008 - val_loss: 1.0589\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0011 - val_loss: 1.0404\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.9994 - val_loss: 1.0284\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0007 - val_loss: 1.0432\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0003 - val_loss: 1.1084\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0006 - val_loss: 1.0429\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.9996 - val_loss: 1.0402\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.9992 - val_loss: 1.0579\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.9992 - val_loss: 1.0664\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.9980 - val_loss: 1.0420\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.9994 - val_loss: 1.1027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.9983 - val_loss: 1.0258\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.9965 - val_loss: 1.0363\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.9962 - val_loss: 1.0788\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.9959 - val_loss: 1.0501\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.9938 - val_loss: 1.0564\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.9921 - val_loss: 1.0702\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.9888 - val_loss: 1.0363\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.9856 - val_loss: 1.0575\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.9784 - val_loss: 1.0473\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.9669 - val_loss: 1.0080\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.9425 - val_loss: 0.9500\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.8628 - val_loss: 0.7947\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.5527 - val_loss: 0.3066\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1959 - val_loss: 0.1567\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1609 - val_loss: 0.1706\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1424 - val_loss: 0.1238\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1245 - val_loss: 0.1054\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1069 - val_loss: 0.0883\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0907 - val_loss: 0.0789\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0785 - val_loss: 0.0632\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0683 - val_loss: 0.0725\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0576 - val_loss: 0.0463\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0552 - val_loss: 0.0828\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0527 - val_loss: 0.0797\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0516 - val_loss: 0.0454\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0492 - val_loss: 0.0390\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0482 - val_loss: 0.0431\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0470 - val_loss: 0.0448\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0470 - val_loss: 0.0517\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0449 - val_loss: 0.0360\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0440 - val_loss: 0.0440\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0427 - val_loss: 0.0411\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0430 - val_loss: 0.0388\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0406 - val_loss: 0.0331\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0414 - val_loss: 0.0372\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0394 - val_loss: 0.0386\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0391 - val_loss: 0.0310\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0384 - val_loss: 0.0433\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0382 - val_loss: 0.0341\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0382 - val_loss: 0.0349\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0370 - val_loss: 0.0364\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0363 - val_loss: 0.0665\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0367 - val_loss: 0.0339\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0355 - val_loss: 0.0282\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0355 - val_loss: 0.0279\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0354 - val_loss: 0.0273\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0332 - val_loss: 0.0272\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0334 - val_loss: 0.0319\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0336 - val_loss: 0.0267\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0327 - val_loss: 0.0302\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0325 - val_loss: 0.0266\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0332 - val_loss: 0.0253\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0321 - val_loss: 0.0425\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0310 - val_loss: 0.0281\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0307 - val_loss: 0.0251\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0304 - val_loss: 0.0243\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0300 - val_loss: 0.0250\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0310 - val_loss: 0.0251\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0295 - val_loss: 0.0310\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0291 - val_loss: 0.0249\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0281 - val_loss: 0.0757\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0284 - val_loss: 0.0221\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0274 - val_loss: 0.0259\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0273 - val_loss: 0.0266\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0268 - val_loss: 0.0485\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0276 - val_loss: 0.0219\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0257 - val_loss: 0.0220\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0254 - val_loss: 0.0206\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0254 - val_loss: 0.0224\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0246 - val_loss: 0.0215\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0262 - val_loss: 0.0199\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0251 - val_loss: 0.0242\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0234 - val_loss: 0.0256\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0241 - val_loss: 0.0197\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0231 - val_loss: 0.0187\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0238 - val_loss: 0.0302\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0228 - val_loss: 0.0184\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0229 - val_loss: 0.0183\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0225 - val_loss: 0.0179\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0229 - val_loss: 0.0183\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0218 - val_loss: 0.0206\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0215 - val_loss: 0.0184\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0210 - val_loss: 0.0182\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0211 - val_loss: 0.0167\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0198 - val_loss: 0.0178\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0203 - val_loss: 0.0165\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0190 - val_loss: 0.0234\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0194 - val_loss: 0.0250\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0193 - val_loss: 0.0162\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0179 - val_loss: 0.0226\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0189 - val_loss: 0.0243\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0177 - val_loss: 0.0146\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0186 - val_loss: 0.0153\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0165 - val_loss: 0.0143\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0171 - val_loss: 0.0173\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model2.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "model2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model2.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "model2.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model2.output,y_train_scaled)\n",
    "listOfVariableTensors = model2.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model2.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "history = model2.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_Trei_6_Capas_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model3.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model3.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "model3.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model3.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model3.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "model3.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model3.output,y_train_scaled)\n",
    "listOfVariableTensors = model3.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model3.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_no_Trei_6_Capas_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.1346 - val_loss: 1.3519\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0664 - val_loss: 1.0964\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0431 - val_loss: 1.1507\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0180 - val_loss: 0.9528\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.9689 - val_loss: 0.8677\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.7228 - val_loss: 0.3932\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2436 - val_loss: 0.1549\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1707 - val_loss: 0.1326\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1513 - val_loss: 0.1278\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.1241 - val_loss: 0.0929\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.1099 - val_loss: 0.0749\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0938 - val_loss: 0.1672\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0910 - val_loss: 0.0563\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0829 - val_loss: 0.0505\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0733 - val_loss: 0.0489\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0726 - val_loss: 0.0449\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0678 - val_loss: 0.0454\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0635 - val_loss: 0.0389\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0608 - val_loss: 0.0495\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0572 - val_loss: 0.0640\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0645 - val_loss: 0.0438\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0527 - val_loss: 0.0432\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0512 - val_loss: 0.0341\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0530 - val_loss: 0.0330\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0486 - val_loss: 0.0389\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0470 - val_loss: 0.0610\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0470 - val_loss: 0.0320\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0513 - val_loss: 0.0462\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0460 - val_loss: 0.0400\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0472 - val_loss: 0.0299\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0461 - val_loss: 0.0326\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0430 - val_loss: 0.0310\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0432 - val_loss: 0.0368\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0410 - val_loss: 0.0293\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0392 - val_loss: 0.0265\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0396 - val_loss: 0.0610\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0400 - val_loss: 0.0313\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0356 - val_loss: 0.0271\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0397 - val_loss: 0.0406\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0364 - val_loss: 0.0373\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0352 - val_loss: 0.0277\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0377 - val_loss: 0.0240\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0357 - val_loss: 0.0538\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0365 - val_loss: 0.0236\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0336 - val_loss: 0.0286\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0326 - val_loss: 0.0311\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0320 - val_loss: 0.0534\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0307 - val_loss: 0.0268\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0311 - val_loss: 0.0216\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0303 - val_loss: 0.0306\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0286 - val_loss: 0.0250\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0298 - val_loss: 0.0288\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0323 - val_loss: 0.0276\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0281 - val_loss: 0.0204\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0304 - val_loss: 0.0199\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0275 - val_loss: 0.0223\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0282 - val_loss: 0.0194\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0253 - val_loss: 0.0668\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0280 - val_loss: 0.0205\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0252 - val_loss: 0.0184\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0271 - val_loss: 0.0200\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0277 - val_loss: 0.0223\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0241 - val_loss: 0.0183\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0260 - val_loss: 0.0256\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0237 - val_loss: 0.0195\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0260 - val_loss: 0.0186\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0239 - val_loss: 0.0181\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0227 - val_loss: 0.0279\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0227 - val_loss: 0.0269\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0229 - val_loss: 0.0267\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0225 - val_loss: 0.0200\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0220 - val_loss: 0.0157\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0202 - val_loss: 0.0416\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0205 - val_loss: 0.0162\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0221 - val_loss: 0.0248\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0203 - val_loss: 0.0197\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0205 - val_loss: 0.0143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0208 - val_loss: 0.0232\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0199 - val_loss: 0.0331\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0208 - val_loss: 0.0137\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0176 - val_loss: 0.0252\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0191 - val_loss: 0.0199\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0189 - val_loss: 0.0158\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0192 - val_loss: 0.0128\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0175 - val_loss: 0.0133\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0170 - val_loss: 0.0134\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0167 - val_loss: 0.0207\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0192 - val_loss: 0.0146\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0164 - val_loss: 0.0220\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0161 - val_loss: 0.0217\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0155 - val_loss: 0.0179\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0152 - val_loss: 0.0118\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0148 - val_loss: 0.0127\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0146 - val_loss: 0.0108\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0172 - val_loss: 0.0109\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0147 - val_loss: 0.0116\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0139 - val_loss: 0.0105\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0133 - val_loss: 0.0168\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0138 - val_loss: 0.0101\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0150 - val_loss: 0.0105\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0148 - val_loss: 0.0196\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0145 - val_loss: 0.0206\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0122 - val_loss: 0.0159\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0115 - val_loss: 0.0136\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0150 - val_loss: 0.0270\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0146 - val_loss: 0.0208\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0143 - val_loss: 0.0097\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0105 - val_loss: 0.0131\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0132 - val_loss: 0.0207\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0125 - val_loss: 0.0118\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0101 - val_loss: 0.0087\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0114 - val_loss: 0.0126\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0098 - val_loss: 0.0083\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0108 - val_loss: 0.0157\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0091 - val_loss: 0.0078\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0098 - val_loss: 0.0077\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0092 - val_loss: 0.0074\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0092 - val_loss: 0.0103\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0089 - val_loss: 0.0074\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0089 - val_loss: 0.0147\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0092 - val_loss: 0.0179\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0097 - val_loss: 0.0071\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0084 - val_loss: 0.0070\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0086 - val_loss: 0.0072\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0097 - val_loss: 0.0082\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0091 - val_loss: 0.0074\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0096 - val_loss: 0.0068\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0085 - val_loss: 0.0067\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0072 - val_loss: 0.0078\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0089 - val_loss: 0.0078\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0091 - val_loss: 0.0115\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0078 - val_loss: 0.0097\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0089 - val_loss: 0.0179\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0079 - val_loss: 0.0216\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0074 - val_loss: 0.0276\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0094 - val_loss: 0.0116\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0076 - val_loss: 0.0066\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0078 - val_loss: 0.0123\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0072 - val_loss: 0.0065\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0085 - val_loss: 0.0065\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0066 - val_loss: 0.0132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0074 - val_loss: 0.0066\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0081 - val_loss: 0.0068\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0069 - val_loss: 0.0180\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0085 - val_loss: 0.0096\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0089 - val_loss: 0.0108\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0081 - val_loss: 0.0074\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0067 - val_loss: 0.0096\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0065 - val_loss: 0.0079\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0066 - val_loss: 0.0055\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0063 - val_loss: 0.0055\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0061 - val_loss: 0.0068\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0072 - val_loss: 0.0098\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0063 - val_loss: 0.0058\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0072 - val_loss: 0.0066\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0075 - val_loss: 0.0056\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0056 - val_loss: 0.0062\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0066 - val_loss: 0.0097\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0060 - val_loss: 0.0051\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0055 - val_loss: 0.0059\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0065 - val_loss: 0.0132\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0074 - val_loss: 0.0064\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0056 - val_loss: 0.0106\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0057 - val_loss: 0.0051\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0069 - val_loss: 0.0073\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0059 - val_loss: 0.0049\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0062 - val_loss: 0.0049\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0057 - val_loss: 0.0073\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0049 - val_loss: 0.0111\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0062 - val_loss: 0.0049\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0055 - val_loss: 0.0079\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0058 - val_loss: 0.0052\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0061 - val_loss: 0.0126\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0047 - val_loss: 0.0060\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0056 - val_loss: 0.0066\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0051 - val_loss: 0.0089\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0059 - val_loss: 0.0050\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0056 - val_loss: 0.0071\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0048 - val_loss: 0.0050\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0054 - val_loss: 0.0098\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0051 - val_loss: 0.0067\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0056 - val_loss: 0.0068\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0054 - val_loss: 0.0044\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0051 - val_loss: 0.0066\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0051 - val_loss: 0.0095\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0043 - val_loss: 0.0073\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0050 - val_loss: 0.0053\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0048 - val_loss: 0.0055\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0051 - val_loss: 0.0071\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0047 - val_loss: 0.0051\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0042 - val_loss: 0.0064\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0053 - val_loss: 0.0043\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0045 - val_loss: 0.0081\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0053 - val_loss: 0.0121\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0047 - val_loss: 0.0053\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0046 - val_loss: 0.0166\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0046 - val_loss: 0.0051\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0044 - val_loss: 0.0062\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0043 - val_loss: 0.0075\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0042 - val_loss: 0.0057\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0044 - val_loss: 0.0066\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0042 - val_loss: 0.0045\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0046 - val_loss: 0.0062\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0048 - val_loss: 0.0051\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model4.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model4.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model4.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model4.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model4.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "model4.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model4.output,y_train_scaled)\n",
    "listOfVariableTensors = model4.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model4.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "history = model4.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_Trei_6_Glorot_Capas_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model5.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model5.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model5.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model5.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model5.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "model5.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model5.output,y_train_scaled)\n",
    "listOfVariableTensors = model5.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model5.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_no_Trei_6_Glorot_Capas_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 175us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 1.0000 - val_loss: 1.0628\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFSJJREFUeJzt3X+w3XV95/HnyyCUrSK/rhRJbKimM6Jto2aRHcetBQuBzho6A7NhtyU6zKQqzLaz3R1j2xm6KjOyM5apU6SlS8bgtA2U1pKxUZoGHOuM/IhKg5G1uSIrMQxEA4jrKgu+94/zyXoIJ/d+7s9zNc/HzJnzPe/v5/v9vM/NTV7nfL/fc5KqQpKkHi8adwOSpB8fhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG7HjLuB+XbqqafWypUrx92GJP1Y+cIXvvCtqpqYbtxPXGisXLmSXbt2jbsNSfqxkuR/9Yzz8JQkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSp20/cJ8LnYuWmv1+UeR7+0K8tyjySNN98pyFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKnbtKGR5KeS3Jvkn5PsSfLfWv3MJPck2ZvkliTHtvpx7fFkW79yaF/va/WvJrlgqL621SaTbBqqj5xDkjQePe80fgCcW1W/BKwG1iY5B7gWuK6qVgFPAFe08VcAT1TVq4Hr2jiSnAWsB14LrAU+mmRZkmXA9cCFwFnAZW0sU8whSRqDaUOjBr7bHr643Qo4F7it1bcAF7flde0xbf15SdLqW6vqB1X1dWASOLvdJqvqoap6BtgKrGvbHGkOSdIYdJ3TaO8I7gceB3YAXwOerKpn25B9wBlt+QzgEYC2/inglOH6YdscqX7KFHMc3t/GJLuS7Dpw4EDPU5IkzUJXaFTVc1W1GljO4J3Ba0YNa/c5wrr5qo/q78aqWlNVayYmJkYNkSTNgxldPVVVTwKfAc4BTkxy6H/+Ww7sb8v7gBUAbf3LgIPD9cO2OVL9W1PMIUkag56rpyaSnNiWjwfeBjwI3AVc0oZtAG5vy9vaY9r6O6uqWn19u7rqTGAVcC9wH7CqXSl1LIOT5dvaNkeaQ5I0Bj3/R/jpwJZ2ldOLgFur6pNJvgJsTfJB4EvATW38TcDHk0wyeIexHqCq9iS5FfgK8CxwZVU9B5DkKuAOYBmwuar2tH299whzSJLGYNrQqKrdwOtH1B9icH7j8Pr3gUuPsK9rgGtG1LcD23vnkCSNh58IlyR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVK3aUMjyYokdyV5MMmeJL/d6n+Y5JtJ7m+3i4a2eV+SySRfTXLBUH1tq00m2TRUPzPJPUn2JrklybGtflx7PNnWr5zPJy9JmpmedxrPAr9bVa8BzgGuTHJWW3ddVa1ut+0Abd164LXAWuCjSZYlWQZcD1wInAVcNrSfa9u+VgFPAFe0+hXAE1X1auC6Nk6SNCbThkZVPVpVX2zLTwMPAmdMsck6YGtV/aCqvg5MAme322RVPVRVzwBbgXVJApwL3Na23wJcPLSvLW35NuC8Nl6SNAYzOqfRDg+9Hrinla5KsjvJ5iQntdoZwCNDm+1rtSPVTwGerKpnD6s/b19t/VNt/OF9bUyyK8muAwcOzOQpSZJmoDs0krwE+Bvgd6rqO8ANwKuA1cCjwIcPDR2xec2iPtW+nl+ourGq1lTVmomJiSmfhyRp9rpCI8mLGQTGX1TV3wJU1WNV9VxV/RD4cwaHn2DwTmHF0ObLgf1T1L8FnJjkmMPqz9tXW/8y4OBMnqAkaf70XD0V4Cbgwar6o6H66UPDfh34clveBqxvVz6dCawC7gXuA1a1K6WOZXCyfFtVFXAXcEnbfgNw+9C+NrTlS4A723hJ0hgcM/0Q3gz8JvBAkvtb7fcYXP20msHhooeB3wKoqj1JbgW+wuDKqyur6jmAJFcBdwDLgM1Vtaft773A1iQfBL7EIKRo9x9PMsngHcb6OTxXSdIcTRsaVfU5Rp9b2D7FNtcA14yobx+1XVU9xI8Obw3Xvw9cOl2PkqTF4SfCJUndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSt2lDI8mKJHcleTDJniS/3eonJ9mRZG+7P6nVk+QjSSaT7E7yhqF9bWjj9ybZMFR/Y5IH2jYfSZKp5pAkjUfPO41ngd+tqtcA5wBXJjkL2ATsrKpVwM72GOBCYFW7bQRugEEAAFcDbwLOBq4eCoEb2thD261t9SPNIUkag2lDo6oeraovtuWngQeBM4B1wJY2bAtwcVteB9xcA3cDJyY5HbgA2FFVB6vqCWAHsLatO6GqPl9VBdx82L5GzSFJGoMZndNIshJ4PXAPcFpVPQqDYAFe3oadATwytNm+Vpuqvm9EnSnmkCSNQXdoJHkJ8DfA71TVd6YaOqJWs6h3S7Ixya4kuw4cODCTTSVJM9AVGklezCAw/qKq/raVH2uHlmj3j7f6PmDF0ObLgf3T1JePqE81x/NU1Y1Vtaaq1kxMTPQ8JUnSLPRcPRXgJuDBqvqjoVXbgENXQG0Abh+qX96uojoHeKodWroDOD/JSe0E+PnAHW3d00nOaXNdfti+Rs0hSRqDYzrGvBn4TeCBJPe32u8BHwJuTXIF8A3g0rZuO3ARMAl8D3gnQFUdTPIB4L427v1VdbAtvxv4GHA88Kl2Y4o5JEljMG1oVNXnGH3eAeC8EeMLuPII+9oMbB5R3wW8bkT926PmkCSNh58IlyR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVK3aUMjyeYkjyf58lDtD5N8M8n97XbR0Lr3JZlM8tUkFwzV17baZJJNQ/Uzk9yTZG+SW5Ic2+rHtceTbf3K+XrSkqTZ6Xmn8TFg7Yj6dVW1ut22AyQ5C1gPvLZt89Eky5IsA64HLgTOAi5rYwGubftaBTwBXNHqVwBPVNWrgevaOEnSGE0bGlX1WeBg5/7WAVur6gdV9XVgEji73Sar6qGqegbYCqxLEuBc4La2/Rbg4qF9bWnLtwHntfGSpDGZyzmNq5LsboevTmq1M4BHhsbsa7Uj1U8BnqyqZw+rP29fbf1TbbwkaUxmGxo3AK8CVgOPAh9u9VHvBGoW9an29QJJNibZlWTXgQMHpupbkjQHswqNqnqsqp6rqh8Cf87g8BMM3imsGBq6HNg/Rf1bwIlJjjms/rx9tfUv4wiHyarqxqpaU1VrJiYmZvOUJEkdZhUaSU4fevjrwKErq7YB69uVT2cCq4B7gfuAVe1KqWMZnCzfVlUF3AVc0rbfANw+tK8NbfkS4M42XpI0JsdMNyDJXwFvBU5Nsg+4GnhrktUMDhc9DPwWQFXtSXIr8BXgWeDKqnqu7ecq4A5gGbC5qva0Kd4LbE3yQeBLwE2tfhPw8SSTDN5hrJ/zs5Ukzcm0oVFVl40o3zSidmj8NcA1I+rbge0j6g/xo8Nbw/XvA5dO158kafH4iXBJUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdZs2NJJsTvJ4ki8P1U5OsiPJ3nZ/UqsnyUeSTCbZneQNQ9tsaOP3JtkwVH9jkgfaNh9JkqnmkCSNT887jY8Baw+rbQJ2VtUqYGd7DHAhsKrdNgI3wCAAgKuBNwFnA1cPhcANbeyh7dZOM4ckaUymDY2q+ixw8LDyOmBLW94CXDxUv7kG7gZOTHI6cAGwo6oOVtUTwA5gbVt3QlV9vqoKuPmwfY2aQ5I0JrM9p3FaVT0K0O5f3upnAI8MjdvXalPV942oTzXHCyTZmGRXkl0HDhyY5VOSJE1nvk+EZ0StZlGfkaq6sarWVNWaiYmJmW4uSeo029B4rB1aot0/3ur7gBVD45YD+6epLx9Rn2oOSdKYzDY0tgGHroDaANw+VL+8XUV1DvBUO7R0B3B+kpPaCfDzgTvauqeTnNOumrr8sH2NmkOSNCbHTDcgyV8BbwVOTbKPwVVQHwJuTXIF8A3g0jZ8O3ARMAl8D3gnQFUdTPIB4L427v1Vdejk+rsZXKF1PPCpdmOKOSRJYzJtaFTVZUdYdd6IsQVceYT9bAY2j6jvAl43ov7tUXNIksbHT4RLkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6jbtf8KkH0+/sOUXFnyOBzY8sOBzAFz/rjsXfI4r//TcBZ9j36Z/WvA5AJZ/6C0LPsfOO1+14HOcd+7XFnwOzZzvNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd3mFBpJHk7yQJL7k+xqtZOT7Eiyt92f1OpJ8pEkk0l2J3nD0H42tPF7k2wYqr+x7X+ybZu59CtJmpv5eKfxK1W1uqrWtMebgJ1VtQrY2R4DXAisareNwA0wCBngauBNwNnA1YeCpo3ZOLTd2nnoV5I0SwtxeGodsKUtbwEuHqrfXAN3AycmOR24ANhRVQer6glgB7C2rTuhqj5fVQXcPLQvSdIYzDU0CviHJF9IsrHVTquqRwHa/ctb/QzgkaFt97XaVPV9I+qSpDGZ6yfC31xV+5O8HNiR5H9OMXbU+YiaRf2FOx4E1kaAV77ylVN3LEmatTm906iq/e3+ceATDM5JPNYOLdHuH2/D9wErhjZfDuyfpr58RH1UHzdW1ZqqWjMxMTGXpyRJmsKsQyPJTyd56aFl4Hzgy8A24NAVUBuA29vyNuDydhXVOcBT7fDVHcD5SU5qJ8DPB+5o655Ock67auryoX1JksZgLoenTgM+0a6CPQb4y6r6dJL7gFuTXAF8A7i0jd8OXARMAt8D3glQVQeTfAC4r417f1UdbMvvBj4GHA98qt0kSWMy69CoqoeAXxpR/zZw3oh6AVceYV+bgc0j6ruA1822R0nS/PIT4ZKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqtuRDI8naJF9NMplk07j7kaSj2ZIOjSTLgOuBC4GzgMuSnDXeriTp6LWkQwM4G5isqoeq6hlgK7BuzD1J0lFrqYfGGcAjQ4/3tZokaQyOGXcD08iIWr1gULIR2NgefjfJV2c536nAt2a5bbdcO+NNFqWvmco7siT7YhY/r6v+bIE6eb7F+Xn9hPx+wU/O79cimktvP9szaKmHxj5gxdDj5cD+wwdV1Y3AjXOdLMmuqloz1/3MN/uaGfuaGfuamaXaFyxOb0v98NR9wKokZyY5FlgPbBtzT5J01FrS7zSq6tkkVwF3AMuAzVW1Z8xtSdJRa0mHBkBVbQe2L9J0cz7EtUDsa2bsa2bsa2aWal+wCL2l6gXnlSVJGmmpn9OQJC0hR3VoJDk5yY4ke9v9SVOMPSHJN5P8yVLoK8nPJvlCkvuT7EnyriXS1+okn2897U7y75dCX23cp5M8meSTC9zPlF99k+S4JLe09fckWbmQ/cygr3+b5ItJnk1yyWL01NnXf07ylfb7tDNJ16Whi9DXu5I80P4Ofm6xvq2i96uVklySpJLM79VUVXXU3oD/Dmxqy5uAa6cY+8fAXwJ/shT6Ao4FjmvLLwEeBl6xBPr6eWBVW34F8Chw4rj7auvOA/4d8MkF7GUZ8DXg59qf0T8DZx025j3An7bl9cAti/A71dPXSuAXgZuBSxa6pxn09SvAv2rL715CP68ThpbfDnx6KfTVxr0U+CxwN7BmPns4qt9pMPhKki1teQtw8ahBSd4InAb8w1Lpq6qeqaoftIfHsTjvGnv6+peq2tuW9wOPAxPj7qv1sxN4eoF76fnqm+F+bwPOSzLqg6yL2ldVPVxVu4EfLnAvM+3rrqr6Xnt4N4PPay2Fvr4z9PCnGfHB43H01XyAwYup7893A0d7aJxWVY8CtPuXHz4gyYuADwP/dSn11XpbkWQ3g69aubb9Iz32vob6O5vBq6GvLaW+FljPV9/8/zFV9SzwFHDKEuhrHGba1xXApxa0o4GuvpJcmeRrDP6B/k9Loa8krwdWVNWCHIZd8pfczlWSfwR+ZsSq3+/cxXuA7VX1yHy+GJyHvqiqR4BfTPIK4O+S3FZVj427r7af04GPAxuqas6vXOerr0XQ89U3XV+PM8/GMWeP7r6S/AawBvjlBe2oTTei9oK+qup64Pok/wH4A2DDOPtqL3KvA96xUA38xIdGVb3tSOuSPJbk9Kp6tP0j9/iIYf8GeEuS9zA4d3Bsku9W1Zz+b4956Gt4X/uT7AHewuBwx1j7SnIC8PfAH1TV3XPpZz77WiQ9X31zaMy+JMcALwMOLoG+xqGrryRvY/AC4ZeHDsuOva8hW4EbFrSjgen6einwOuAz7UXuzwDbkry9qnbNRwNH++GpbfzolcEG4PbDB1TVf6yqV1bVSuC/ADfPNTDmo68ky5Mc35ZPAt4MzPaLGuezr2OBTzD4Of31AvfT3dci6vnqm+F+LwHurHb2csx9jcO0fbXDLX8GvL2qFusFQU9fq4Ye/hqwd9x9VdVTVXVqVa1s/2bdzeDnNi+BcWiSo/bG4DjyTgZ/2DuBk1t9DfA/Rox/B4tz9dS0fQG/CuxmcPXEbmDjEunrN4D/C9w/dFs97r7a438CDgD/h8ErtgsWqJ+LgH9hcC7n91vt/Qz+8gL8FPDXwCRwL/BzC/1n19nXv24/l/8NfBvYs0T6+kfgsaHfp21LpK8/Bva0nu4CXrsU+jps7GeY56un/ES4JKnb0X54SpI0A4aGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuv0/xW1Ua5KeioMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='relu'))\n",
    "model6.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "model6.add(Dense(256,  kernel_initializer='uniform',activation='relu'))\n",
    "model6.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "model6.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "model6.add(Dense(1, kernel_initializer='uniform',activation='relu'))\n",
    "model6.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model6.output,y_train_scaled)\n",
    "listOfVariableTensors = model6.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model6.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "history = model6.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_Relu_Trei_Uni_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='relu'))\n",
    "model7.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "model7.add(Dense(256,  kernel_initializer='uniform',activation='relu'))\n",
    "model7.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "model7.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "model7.add(Dense(1, kernel_initializer='uniform',activation='relu'))\n",
    "model7.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model7.output,y_train_scaled)\n",
    "listOfVariableTensors = model7.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model7.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_Relu_no_Uni_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 1.0105 - val_loss: 1.0628\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0000 - val_loss: 1.0628\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFTlJREFUeJzt3X+MXeV95/H3p/ZC221YTBkowVATZCJBuuuEEWVVkdImgEGrQKqkNWqLm2XXIYHVVvtDdTYrESVbKf2RRotKyZLGwqwafiQNwWrIEsdJSnaFE48LNT9S4oHQMGDZDiRpqnRpId/94z6TvfUZe67vHc8dm/dLurrnfs9zznkej2c+9zznzJ1UFZIk9fuRcXdAkrT0GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdSwfdweGdfLJJ9eqVavG3Q1JOqrs3LnzW1U1MV+7ozYcVq1axdTU1Li7IUlHlSR/PUg7p5UkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUMW84JNmUZF+SR/tqdyV5uD2eTvJwq69K8nd96z7St835SR5JMp3kpiRp9ZOSbE2yuz2vOBIDlSQNbpAzh9uAtf2FqvqVqlpTVWuAPwU+1bf6ydl1VXVdX/0WYAOwuj1m97kR2FZVq4Ft7bUkaYzmDYeqegB4Ya517d3/LwN3HGofSU4DTqiqB6uqgNuBq9rqK4HNbXlzX12SNCajXnO4CNhbVbv7amcleSjJnye5qNVOB2b62sy0GsCpVbUHoD2fMmKfJEkjGvUvwV3NPz5r2AOcWVXPJzkf+HSS84DMsW0d7sGSbKA3NcWZZ545RHclSYMY+swhyXLgl4C7ZmtV9WJVPd+WdwJPAufQO1NY2bf5SuC5try3TTvNTj/tO9gxq+rWqpqsqsmJiXn/BKokaUijTCu9GfirqvrhdFGSiSTL2vJr6F14fqpNF30vyYXtOsU1wL1tsy3A+ra8vq8uSRqTQW5lvQN4EHhtkpkk17ZV6+heiH4jsCvJXwKfBK6rqtmL2e8C/hiYpndG8dlW/yBwSZLdwCXttSRpjNK7eejoMzk5WVNTU+PuhiQdVZLsrKrJ+dr5G9KSpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVLHvOGQZFOSfUke7au9L8mzSR5ujyv61r0nyXSSJ5Jc1ldf22rTSTb21c9K8pUku5PcleS4hRygJOnwDXLmcBuwdo76h6tqTXvcB5DkXGAdcF7b5o+SLEuyDLgZuBw4F7i6tQX4nbav1cC3gWtHGZAkaXTzhkNVPQC8MOD+rgTurKoXq+obwDRwQXtMV9VTVfX3wJ3AlUkC/CLwybb9ZuCqwxyDJGmBjXLN4YYku9q004pWOx14pq/NTKsdrP6TwHeq6qUD6pKkMRo2HG4BzgbWAHuAD7V65mhbQ9TnlGRDkqkkU/v37z+8HkuSBjZUOFTV3qp6uap+AHyU3rQR9N75n9HXdCXw3CHq3wJOTLL8gPrBjntrVU1W1eTExMQwXZckDWCocEhyWt/LtwKzdzJtAdYlOT7JWcBq4KvADmB1uzPpOHoXrbdUVQFfBN7Wtl8P3DtMnyRJC2f5fA2S3AFcDJycZAa4Ebg4yRp6U0BPA+8EqKrHktwNPA68BFxfVS+3/dwA3A8sAzZV1WPtEL8F3JnkvwEPAR9bsNFJkoaS3pv3o8/k5GRNTU2NuxuSdFRJsrOqJudr529IS5I6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeqYNxySbEqyL8mjfbXfS/JXSXYluSfJia2+KsnfJXm4PT7St835SR5JMp3kpiRp9ZOSbE2yuz2vOBIDlSQNbpAzh9uAtQfUtgKvq6p/DnwdeE/fuierak17XNdXvwXYAKxuj9l9bgS2VdVqYFt7LUkao3nDoaoeAF44oPa5qnqpvdwOrDzUPpKcBpxQVQ9WVQG3A1e11VcCm9vy5r66JGlMFuKaw78GPtv3+qwkDyX58yQXtdrpwExfm5lWAzi1qvYAtOdTFqBPkqQRLB9l4yTvBV4C/qSV9gBnVtXzSc4HPp3kPCBzbF5DHG8DvakpzjzzzOE6LUma19BnDknWA/8K+NU2VURVvVhVz7flncCTwDn0zhT6p55WAs+15b1t2ml2+mnfwY5ZVbdW1WRVTU5MTAzbdUnSPIYKhyRrgd8C3lJV3++rTyRZ1pZfQ+/C81Ntuuh7SS5sdyldA9zbNtsCrG/L6/vqkqQxmXdaKckdwMXAyUlmgBvp3Z10PLC13ZG6vd2Z9Ebg/UleAl4Grquq2YvZ76J359OP0btGMXud4oPA3UmuBb4JvH1BRiZJGlrajNBRZ3JysqampsbdDUk6qiTZWVWT87XzN6QlSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqSOgcIhyaYk+5I82lc7KcnWJLvb84pWT5Kbkkwn2ZXkDX3brG/tdydZ31c/P8kjbZubkmQhBylJOjyDnjncBqw9oLYR2FZVq4Ft7TXA5cDq9tgA3AK9MAFuBH4WuAC4cTZQWpsNfdsdeCxJ0iIaKByq6gHghQPKVwKb2/Jm4Kq++u3Vsx04MclpwGXA1qp6oaq+DWwF1rZ1J1TVg1VVwO19+5IkjcEo1xxOrao9AO35lFY/HXimr91Mqx2qPjNHvSPJhiRTSab2798/QtclSYdyJC5Iz3W9oIaod4tVt1bVZFVNTkxMjNBFSdKhjBIOe9uUEO15X6vPAGf0tVsJPDdPfeUcdUnSmIwSDluA2TuO1gP39tWvaXctXQh8t0073Q9cmmRFuxB9KXB/W/e9JBe2u5Su6duXJGkMlg/SKMkdwMXAyUlm6N119EHg7iTXAt8E3t6a3wdcAUwD3wfeAVBVLyT5ALCjtXt/Vc1e5H4XvTuifgz4bHtIksYkvRuEjj6Tk5M1NTU17m5I0lElyc6qmpyvnb8hLUnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKlj6HBI8tokD/c9/ibJbyZ5X5Jn++pX9G3zniTTSZ5IcllffW2rTSfZOOqgJEmjWT7shlX1BLAGIMky4FngHuAdwIer6vf72yc5F1gHnAe8Gvh8knPa6puBS4AZYEeSLVX1+LB9kySNZuhwOMCbgCer6q+THKzNlcCdVfUi8I0k08AFbd10VT0FkOTO1tZwkKQxWahrDuuAO/pe35BkV5JNSVa02unAM31tZlrtYHVJ0piMHA5JjgPeAnyilW4BzqY35bQH+NBs0zk2r0PU5zrWhiRTSab2798/Ur8lSQe3EGcOlwN/UVV7Aapqb1W9XFU/AD7K/586mgHO6NtuJfDcIeodVXVrVU1W1eTExMQCdF2SNJeFCIer6ZtSSnJa37q3Ao+25S3AuiTHJzkLWA18FdgBrE5yVjsLWdfaSpLGZKQL0kl+nN5dRu/sK/9ukjX0poaenl1XVY8luZveheaXgOur6uW2nxuA+4FlwKaqemyUfkmSRpOqOaf3l7zJycmampoadzck6aiSZGdVTc7Xzt+QliR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktQxcjgkeTrJI0keTjLVaicl2Zpkd3te0epJclOS6SS7kryhbz/rW/vdSdaP2i9J0vAW6szhF6pqTd8frd4IbKuq1cC29hrgcmB1e2wAboFemAA3Aj8LXADcOBsokqTFd6Smla4ENrflzcBVffXbq2c7cGKS04DLgK1V9UJVfRvYCqw9Qn2TJM1jIcKhgM8l2ZlkQ6udWlV7ANrzKa1+OvBM37YzrXawuiRpDBYiHH6uqt5Ab8ro+iRvPETbzFGrQ9T/8cbJhiRTSab2798/XG+lMVm18TPj7oI0sJHDoaqea8/7gHvoXTPY26aLaM/7WvMZ4Iy+zVcCzx2ifuCxbq2qyaqanJiYGLXrkqSDGCkckvzTJK+aXQYuBR4FtgCzdxytB+5ty1uAa9pdSxcC323TTvcDlyZZ0S5EX9pqkqQxWD7i9qcC9ySZ3dfHq+p/JdkB3J3kWuCbwNtb+/uAK4Bp4PvAOwCq6oUkHwB2tHbvr6oXRuybJGlII4VDVT0F/Is56s8Db5qjXsD1B9nXJmDTKP2RJC0Mf0NaktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqGDockpyR5ItJvpbksST/vtXfl+TZJA+3xxV927wnyXSSJ5Jc1ldf22rTSTaONiRJ0qiWj7DtS8B/rKq/SPIqYGeSrW3dh6vq9/sbJzkXWAecB7wa+HySc9rqm4FLgBlgR5ItVfX4CH2TJI1g6HCoqj3Anrb8vSRfA04/xCZXAndW1YvAN5JMAxe0ddNV9RRAkjtbW8NBksZkQa45JFkFvB74SivdkGRXkk1JVrTa6cAzfZvNtNrB6nMdZ0OSqSRT+/fvX4iuS5LmMHI4JPkJ4E+B36yqvwFuAc4G1tA7s/jQbNM5Nq9D1LvFqlurarKqJicmJkbtuiTpIEa55kCSf0IvGP6kqj4FUFV7+9Z/FPiz9nIGOKNv85XAc235YHVJ0hiMcrdSgI8BX6uqP+irn9bX7K3Ao215C7AuyfFJzgJWA18FdgCrk5yV5Dh6F623DNsvSdLoRjlz+Dng14FHkjzcav8FuDrJGnpTQ08D7wSoqseS3E3vQvNLwPVV9TJAkhuA+4FlwKaqemyEfkmSRjTK3Ur/m7mvF9x3iG1+G/jtOer3HWo7SdLi8jekJUkdhoMkqcNwkBbBzdd9YajtVm38zAL3RBqM4SBJ6jAcJEkdhoMkqcNwkIbgtQAd6wwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQTqGecuthmU4SJI6DAcdU4Z5p7yQ765/ZvPPLNi+pHEyHKQjYNsXzh53F6SRGA7SmC21sw2vUwgMB0nSHJZMOCRZm+SJJNNJNo67P9JiO5qnojzbOPYsiXBIsgy4GbgcOBe4Osm54+2Vxu1o+IEzs/HLR2zfw/71OGkhLIlwAC4Apqvqqar6e+BO4Mox90l9joYf1KM68Af97LWApfaOfmbjlw96neJoCpRx31mmQ1sq4XA68Ezf65lWO2ot1n/8pfoNdrAfUod6p32obeb6YbjtC2cPfJz+H/RH0w/QhbIQ/4bDbDNIwB7pUB5HoCzV78vDkaoadx9I8nbgsqr6N+31rwMXVNW/O6DdBmBDe/la4IkRD30y8K0R97FUHEtjgWNrPMfSWODYGs+xNBYYbDw/XVUT8+1o+cL0Z2QzwBl9r1cCzx3YqKpuBW5dqIMmmaqqyYXa3zgdS2OBY2s8x9JY4Ngaz7E0FljY8SyVaaUdwOokZyU5DlgHbBlznyTpFWtJnDlU1UtJbgDuB5YBm6rqsTF3S5JesZZEOABU1X3AfYt82AWboloCjqWxwLE1nmNpLHBsjedYGgss5LT7UrggLUlaWpbKNQdJ0hLyigqHJCcl2Zpkd3tecZB2Zyb5XJKvJXk8yarF7en8Bh1La3tCkmeT/OFi9vFwDDKeJGuSPJjksSS7kvzKOPp6MPN9BEyS45Pc1dZ/ZSn+v+o3wHj+Q/v+2JVkW5KfHkc/BzHox/MkeVuSSrJk72AaZCxJfrl9bR5L8vGhDlRVr5gH8LvAxra8Efidg7T7EnBJW/4J4MfH3fdhx9LW/3fg48Afjrvfo4wHOAdY3ZZfDewBThx331t/lgFPAq8BjgP+Ejj3gDbvBj7SltcBd4273yOO5xdmvzeAdy3V8QwyltbuVcADwHZgctz9HuHrshp4CFjRXp8yzLFeUWcO9D6SY3Nb3gxcdWCD9plOy6tqK0BV/W1VfX/xujiweccCkOR84FTgc4vUr2HNO56q+npV7W7LzwH7gHl/mWeRDPIRMP1j/CTwpiRZxD4ejnnHU1Vf7Pve2E7v95OWokE/nucD9N6k/N/F7NxhGmQs/xa4uaq+DVBV+4Y50CstHE6tqj0A7fmUOdqcA3wnyaeSPJTk99oHAy41844lyY8AHwL+8yL3bRiDfG1+KMkF9N45PbkIfRvEIB8B88M2VfUS8F3gJxeld4fvcD/S5lrgs0e0R8ObdyxJXg+cUVV/tpgdG8IgX5dzgHOS/J8k25OsHeZAS+ZW1oWS5PPAT82x6r0D7mI5cBHweuCbwF3AbwAfW4j+HY4FGMu7gfuq6pml8AZ1AcYzu5/TgP8JrK+qHyxE3xbAXP/AB94KOEibpWLgvib5NWAS+Pkj2qPhHXIs7U3Uh+l9ny91g3xdltObWrqY3tncl5O8rqq+czgHOubCoarefLB1SfYmOa2q9rQfMHOdbs0AD1XVU22bTwMXMoZwWICx/EvgoiTvpnft5Lgkf1tVY/l7GQswHpKcAHwG+K9Vtf0IdXUYg3wEzGybmSTLgX8GvLA43TtsA32kTZI30wv3n6+qFxepb4drvrG8Cngd8KX2JuqngC1J3lJVU4vWy8EM+v9se1X9A/CNJE/QC4sdh3OgV9q00hZgfVteD9w7R5sdwIoks3PZvwg8vgh9O1zzjqWqfrWqzqyqVcB/Am4fVzAMYN7xtI9WuYfeOD6xiH0bxCAfAdM/xrcBX6h2xXAJmnc8bSrmfwBvGXZee5EccixV9d2qOrmqVrXvle30xrTUggEG+3/2aXo3C5DkZHrTTE8d9pHGffV9MR/05ne3Abvb80mtPgn8cV+7S4BdwCPAbcBx4+77sGPpa/8bLO27leYdD/BrwD8AD/c91oy7731juAL4Or3rIO9ttffT+0ED8KPAJ4Bp4KvAa8bd5xHH83lgb9/XYsu4+zzsWA5o+yWW6N1KA35dAvwBvTe1jwDrhjmOvyEtSep4pU0rSZIGYDhIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqSO/wfatwTZEjz5BAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model8 = Sequential()\n",
    "model8.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='relu'))\n",
    "model8.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "model8.add(Dense(256,  kernel_initializer='he_uniform',activation='relu'))\n",
    "model8.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "model8.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "model8.add(Dense(1, kernel_initializer='he_uniform',activation='relu'))\n",
    "model8.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model8.output,y_train_scaled)\n",
    "listOfVariableTensors = model8.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model8.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "history = model8.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_Relu_Trei_he_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAD4BJREFUeJzt3H+s3Xddx/Hny9UhonMd6+bcBgVSE4fTAdeyhERAyNbNSGeyxWFgDZmp4PBH9A+rkMwM/wATNS6Z0ykLrVHGQMkaGNRSloAJg93h3A8QWhDZtc1a6BiLM+Lg7R/3c/Wkn9t7z/11zj3t85GcnO/3fT7f731/Tnvv65zv93tOqgpJkgZ937gbkCStP4aDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOhvG3cBynXvuubV58+ZxtyFJE+XBBx/8RlVtWmzcxIbD5s2bmZ6eHncbkjRRkvz7MOM8rCRJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6iwaDkkuTnJfki8meSzJb7b6OUn2JznY7je2epLcmuRQkoeTvHxgXzva+INJdgzUX5HkkbbNrUmyFpOVJA1nmHcOzwK/U1U/AVwO3JTkEmAXcKCqtgAH2jrAVcCWdtsJ3A6zYQLcDLwS2ArcPBcobczOge22rXxqkqTlWjQcqupIVX2+LT8NfBG4ENgO7G7DdgPXtOXtwJ6adT9wdpILgCuB/VV1vKqeBPYD29pjZ1XVZ6qqgD0D+5IkjcGSzjkk2Qy8DPgscH5VHYHZAAHOa8MuBB4f2Gym1Raqz8xTn+/n70wynWT62LFjS2ldkrQEQ4dDkh8C/h74rar69kJD56nVMup9seqOqpqqqqlNmzYt1rIkaZmGCock389sMPxtVf1DKz/RDgnR7o+2+gxw8cDmFwGHF6lfNE9dkjQmw1ytFOC9wBer6k8GHtoLzF1xtAO4Z6B+Q7tq6XLgqXbYaR9wRZKN7UT0FcC+9tjTSS5vP+uGgX1JksZgwxBjXgW8GXgkyUOt9vvAu4G7k9wIfB24rj12L3A1cAh4BngLQFUdT/Iu4IE27paqOt6W3wa8D3gu8LF2kySNSWYvEJo8U1NTNT09Pe42JGmiJHmwqqYWG+cnpCVJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJnUXDIcmdSY4meXSg9gdJ/iPJQ+129cBjv5fkUJIvJblyoL6t1Q4l2TVQf1GSzyY5mOQDSc5czQlKkpZumHcO7wO2zVP/06q6rN3uBUhyCXA98NK2zZ8nOSPJGcBtwFXAJcAb21iA97R9bQGeBG5cyYQkSSu3aDhU1aeA40PubztwV1X9d1X9G3AI2Npuh6rqq1X1HeAuYHuSAD8HfKhtvxu4ZolzkCStspWcc3h7kofbYaeNrXYh8PjAmJlWO1n9+cC3qurZE+qSpDFabjjcDrwEuAw4Avxxq2eesbWM+ryS7EwynWT62LFjS+tYkjS0ZYVDVT1RVd+tqu8Bf8XsYSOYfeV/8cDQi4DDC9S/AZydZMMJ9ZP93DuqaqqqpjZt2rSc1iVJQ1hWOCS5YGD1F4G5K5n2AtcneU6SFwFbgM8BDwBb2pVJZzJ70npvVRVwH3Bt234HcM9yepIkrZ4Niw1I8n7gNcC5SWaAm4HXJLmM2UNAXwN+FaCqHktyN/AF4Fngpqr6btvP24F9wBnAnVX1WPsRvwvcleQPgX8G3rtqs5MkLUtmX7xPnqmpqZqenh53G5I0UZI8WFVTi43zE9KSpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqLBoOSe5McjTJowO1c5LsT3Kw3W9s9SS5NcmhJA8nefnANjva+INJdgzUX5HkkbbNrUmy2pOUJC3NMO8c3gdsO6G2CzhQVVuAA20d4CpgS7vtBG6H2TABbgZeCWwFbp4LlDZm58B2J/4sSdKILRoOVfUp4PgJ5e3A7ra8G7hmoL6nZt0PnJ3kAuBKYH9VHa+qJ4H9wLb22FlV9ZmqKmDPwL4kSWOy3HMO51fVEYB2f16rXwg8PjBuptUWqs/MU5ckjdFqn5Ce73xBLaM+/86TnUmmk0wfO3ZsmS1Kkhaz3HB4oh0Sot0fbfUZ4OKBcRcBhxepXzRPfV5VdUdVTVXV1KZNm5bZuiRpMcsNh73A3BVHO4B7Buo3tKuWLgeeaoed9gFXJNnYTkRfAexrjz2d5PJ2ldINA/uSJI3JhsUGJHk/8Brg3CQzzF519G7g7iQ3Al8HrmvD7wWuBg4BzwBvAaiq40neBTzQxt1SVXMnud/G7BVRzwU+1m6SpDHK7EVCk2dqaqqmp6fH3YYkTZQkD1bV1GLj/IS0JKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOisKhyRfS/JIkoeSTLfaOUn2JznY7je2epLcmuRQkoeTvHxgPzva+INJdqxsSpKklVqNdw6vrarLqmqqre8CDlTVFuBAWwe4CtjSbjuB22E2TICbgVcCW4Gb5wJFkjQea3FYaTuwuy3vBq4ZqO+pWfcDZye5ALgS2F9Vx6vqSWA/sG0N+pIkDWml4VDAPyZ5MMnOVju/qo4AtPvzWv1C4PGBbWda7WR1SdKYbFjh9q+qqsNJzgP2J/nXBcZmnlotUO93MBtAOwFe8IIXLLVXSdKQVvTOoaoOt/ujwIeZPWfwRDtcRLs/2obPABcPbH4RcHiB+nw/746qmqqqqU2bNq2kdUnSApYdDkmel+SH55aBK4BHgb3A3BVHO4B72vJe4IZ21dLlwFPtsNM+4IokG9uJ6CtaTTqpzbs+Ou4WpFPaSg4rnQ98OMncfv6uqj6e5AHg7iQ3Al8Hrmvj7wWuBg4BzwBvAaiq40neBTzQxt1SVcdX0JckaYWWHQ5V9VXgp+epfxN43Tz1Am46yb7uBO5cbi+SpNXlJ6QlSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRx0yvArNaTVYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhoTflleNJkMhwkSR3DQZLUMRwkSR3DQRrgORJpluEgSeoYDlq3Lt196bhbkE5bhoO0CA816XRkOEiSOoaD1rUDn3zJuFuQTkuGgySpYzhIkjqGgybWzK5Pj7sF6ZRlOAjwipy15vOrSWM4SJI66yYckmxL8qUkh5LsGnc/Go3b3vrJcbcwNr6b0Hq2LsIhyRnAbcBVwCXAG5NcMt6uNMkWCh0/eS0tbl2EA7AVOFRVX62q7wB3AdvH3JOGtNgf20k4cTw4h0nod47vPrRW1ks4XAg8PrA+02prbq1+uZay3xM/6HXp7ktP+uGvzbs+uuAfr8VeMS/0obLl7hcm78NqS+137rmZ73mYq833/A7W5nt+F9rvQv0uFMjr4f/0etivViZVNe4eSHIdcGVV/UpbfzOwtap+/YRxO4GdbfUngUdH2uj6cS7wjXE3MQbO+/TivNfGC6tq02KDNqxhA0sxA1w8sH4RcPjEQVV1B3AHQJLpqpoaTXvry+k6d+d9enHe47VeDis9AGxJ8qIkZwLXA3vH3JMknbbWxTuHqno2yduBfcAZwJ1V9diY25Kk09a6CAeAqroXuHcJm9yxVr1MgNN17s779OK8x2hdnJCWJK0v6+WcgyRpHZmYcEhyTpL9SQ62+43zjLksyWeSPJbk4SS/NI5eV9swc2/jPp7kW0k+MuoeV9NiX6WS5DlJPtAe/2ySzaPvcvUNMe+fTfL5JM8muXYcPa6FIeb920m+0H6nDyR54Tj6XG1DzPutSR5J8lCSfxr5t0ZU1UTcgD8CdrXlXcB75hnz48CWtvxjwBHg7HH3Poq5t8deB/wC8JFx97yCuZ4BfAV4MXAm8C/AJSeM+TXgL9ry9cAHxt33iOa9GfgpYA9w7bh7HuG8Xwv8YFt+22n0733WwPIbgI+PsseJeefA7Ndp7G7Lu4FrThxQVV+uqoNt+TBwFFj0wx4TYNG5A1TVAeDpUTW1Rob5KpXB5+NDwOuSZIQ9roVF511VX6uqh4HvjaPBNTLMvO+rqmfa6v3Mfg5q0g0z728PrD4PGOkJ4kkKh/Or6ghAuz9vocFJtjKbyF8ZQW9rbUlzn3DDfJXK/42pqmeBp4Dnj6S7tTO2r5AZs6XO+0bgY2va0WgMNe8kNyX5CrNHD35jRL0B6+hSVoAknwB+dJ6H3rHE/VwA/A2wo6om4lXWas39FDDfO4ATXzENM2bSnIpzGsbQ807yJmAKePWadjQaQ827qm4Dbkvyy8A7gR1r3dicdRUOVfX6kz2W5IkkF1TVkfbH/+hJxp0FfBR4Z1Xdv0atrrrVmPspYpivUpkbM5NkA/AjwPHRtLdmhvoKmVPQUPNO8npmXyi9uqr+e0S9raWl/nvfBdy+ph2dYJIOK+3l/1NzB3DPiQPaV298GNhTVR8cYW9rbdG5n0KG+SqVwefjWuCT1c7aTbDT9StkFp13kpcBfwm8oapOlRdGw8x7y8DqzwMHR9jfRF2t9HzgQHuCDgDntPoU8Ndt+U3A/wAPDdwuG3fvo5h7W/80cAz4L2ZfmVw57t6XOd+rgS8ze77oHa12C7N/HAB+APggcAj4HPDicfc8onn/TPt3/U/gm8Bj4+55RPP+BPDEwO/03nH3PKJ5/xnwWJvzfcBLR9mfn5CWJHUm6bCSJGlEDAdJUsdwkCR1DAdJUsdwkCR1DAdJUsdwkCR1DAdJUud/AXe3Alm26g1iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model9 = Sequential()\n",
    "model9.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='relu'))\n",
    "model9.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "model9.add(Dense(256,  kernel_initializer='he_uniform',activation='relu'))\n",
    "model9.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "model9.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "model9.add(Dense(1, kernel_initializer='he_uniform',activation='relu'))\n",
    "model9.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model9.output,y_train_scaled)\n",
    "listOfVariableTensors = model9.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model9.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_Relu_no_Trei_he_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 1.4532 - val_loss: 0.1765\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1250 - val_loss: 0.0552\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0464 - val_loss: 0.0261\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0245 - val_loss: 0.0186\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0169 - val_loss: 0.0152\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0131 - val_loss: 0.0116\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0090 - val_loss: 0.0093\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0069 - val_loss: 0.0073\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0064 - val_loss: 0.0066\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0057 - val_loss: 0.0068\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0052 - val_loss: 0.0056\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0046 - val_loss: 0.0059\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0038 - val_loss: 0.0063\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0036 - val_loss: 0.0048\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0032 - val_loss: 0.0044\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0030 - val_loss: 0.0046\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0024 - val_loss: 0.0038\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0023 - val_loss: 0.0036\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0023 - val_loss: 0.0040\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0023 - val_loss: 0.0038\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0022 - val_loss: 0.0039\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0021 - val_loss: 0.0036\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0021 - val_loss: 0.0035\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0021 - val_loss: 0.0036\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0021 - val_loss: 0.0036\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0020 - val_loss: 0.0032\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0019 - val_loss: 0.0034\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0018 - val_loss: 0.0033\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0017 - val_loss: 0.0036\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0017 - val_loss: 0.0032\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0016 - val_loss: 0.0029\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0016 - val_loss: 0.0034\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0016 - val_loss: 0.0034\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0015 - val_loss: 0.0036\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0015 - val_loss: 0.0035\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0015 - val_loss: 0.0033\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0014 - val_loss: 0.0037\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0014 - val_loss: 0.0029\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0014 - val_loss: 0.0031\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0014 - val_loss: 0.0035\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0013 - val_loss: 0.0026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0013 - val_loss: 0.0032\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0012 - val_loss: 0.0037\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0012 - val_loss: 0.0026\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0012 - val_loss: 0.0032\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0011 - val_loss: 0.0030\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0011 - val_loss: 0.0030\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0010 - val_loss: 0.0031\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0010 - val_loss: 0.0032\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0010 - val_loss: 0.0028\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0010 - val_loss: 0.0026\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0010 - val_loss: 0.0026\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0010 - val_loss: 0.0025\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0010 - val_loss: 0.0024\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 9.8833e-04 - val_loss: 0.0025\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 9.8287e-04 - val_loss: 0.0025\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0010 - val_loss: 0.0031\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 9.8650e-04 - val_loss: 0.0024\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 9.8455e-04 - val_loss: 0.0026\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0010 - val_loss: 0.0027\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 9.6750e-04 - val_loss: 0.0025\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 9.6773e-04 - val_loss: 0.0027\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 9.5786e-04 - val_loss: 0.0026\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 9.5875e-04 - val_loss: 0.0027\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 9.5983e-04 - val_loss: 0.0025\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 9.4016e-04 - val_loss: 0.0027\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 9.3150e-04 - val_loss: 0.0025\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 9.1580e-04 - val_loss: 0.0024\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 9.2533e-04 - val_loss: 0.0024\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 9.6210e-04 - val_loss: 0.0028\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 9.2022e-04 - val_loss: 0.0024\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 9.1451e-04 - val_loss: 0.0025\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 9.2272e-04 - val_loss: 0.0024\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 9.1320e-04 - val_loss: 0.0025\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 9.4145e-04 - val_loss: 0.0025\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 8.8632e-04 - val_loss: 0.0024\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 9.0897e-04 - val_loss: 0.0025\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 8.9614e-04 - val_loss: 0.0024\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 8.9683e-04 - val_loss: 0.0024\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 8.7933e-04 - val_loss: 0.0024\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 8.8938e-04 - val_loss: 0.0027\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 9.0056e-04 - val_loss: 0.0038\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 8.9251e-04 - val_loss: 0.0024\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 8.9084e-04 - val_loss: 0.0024\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 8.6090e-04 - val_loss: 0.0025\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 8.6482e-04 - val_loss: 0.0025\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 8.6414e-04 - val_loss: 0.0025\n",
      "Epoch 153/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 201us/step - loss: 8.5237e-04 - val_loss: 0.0025\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 8.4222e-04 - val_loss: 0.0027\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 8.4487e-04 - val_loss: 0.0030\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 8.6336e-04 - val_loss: 0.0024\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 8.3849e-04 - val_loss: 0.0024\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 8.5202e-04 - val_loss: 0.0024\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 8.2908e-04 - val_loss: 0.0024\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 8.4682e-04 - val_loss: 0.0027\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 8.3801e-04 - val_loss: 0.0025\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 8.1772e-04 - val_loss: 0.0024\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 8.3336e-04 - val_loss: 0.0025\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 8.4906e-04 - val_loss: 0.0026\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 8.4075e-04 - val_loss: 0.0027\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 8.3955e-04 - val_loss: 0.0023\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 8.4642e-04 - val_loss: 0.0026\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 8.1257e-04 - val_loss: 0.0024\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 8.0170e-04 - val_loss: 0.0028\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 8.1645e-04 - val_loss: 0.0023\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 8.0569e-04 - val_loss: 0.0024\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 8.2617e-04 - val_loss: 0.0024\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 8.1106e-04 - val_loss: 0.0024\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 8.2816e-04 - val_loss: 0.0024\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 8.1121e-04 - val_loss: 0.0024\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.7477e-04 - val_loss: 0.0025\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 7.9176e-04 - val_loss: 0.0024\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.8564e-04 - val_loss: 0.0024\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 7.9703e-04 - val_loss: 0.0023\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 7.6945e-04 - val_loss: 0.0024\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 7.7890e-04 - val_loss: 0.0024\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.6673e-04 - val_loss: 0.0023\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.7825e-04 - val_loss: 0.0024\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.6845e-04 - val_loss: 0.0024\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.8976e-04 - val_loss: 0.0024\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.6936e-04 - val_loss: 0.0024\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 7.5506e-04 - val_loss: 0.0024\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.6474e-04 - val_loss: 0.0030\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 7.6749e-04 - val_loss: 0.0024\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.5645e-04 - val_loss: 0.0026\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.5455e-04 - val_loss: 0.0024\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.5595e-04 - val_loss: 0.0026\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 7.6301e-04 - val_loss: 0.0024\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.3400e-04 - val_loss: 0.0025\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.6508e-04 - val_loss: 0.0023\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 7.3920e-04 - val_loss: 0.0024\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 7.4268e-04 - val_loss: 0.0023\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.3240e-04 - val_loss: 0.0025\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.3300e-04 - val_loss: 0.0024\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 7.4637e-04 - val_loss: 0.0024\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.3696e-04 - val_loss: 0.0024\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 7.2875e-04 - val_loss: 0.0024\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.3311e-04 - val_loss: 0.0024\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 7.2694e-04 - val_loss: 0.0024\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 7.3742e-04 - val_loss: 0.0024\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.2112e-04 - val_loss: 0.0025\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.1993e-04 - val_loss: 0.0023\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.2760e-04 - val_loss: 0.0026\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 7.2973e-04 - val_loss: 0.0024\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 7.0465e-04 - val_loss: 0.0024\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.1209e-04 - val_loss: 0.0025\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.1975e-04 - val_loss: 0.0023\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 7.0748e-04 - val_loss: 0.0024\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 7.1611e-04 - val_loss: 0.0026\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 7.2097e-04 - val_loss: 0.0022\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 6.8705e-04 - val_loss: 0.0024\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 7.2010e-04 - val_loss: 0.0024\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 7.1421e-04 - val_loss: 0.0023\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 6.8605e-04 - val_loss: 0.0023\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 6.8931e-04 - val_loss: 0.0023\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 6.8785e-04 - val_loss: 0.0025\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 7.0951e-04 - val_loss: 0.0022\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 7.0045e-04 - val_loss: 0.0023\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 6.8755e-04 - val_loss: 0.0027\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 6.8220e-04 - val_loss: 0.0023\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 6.7891e-04 - val_loss: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 6.7491e-04 - val_loss: 0.0023\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.6989e-04 - val_loss: 0.0023\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 6.8451e-04 - val_loss: 0.0023\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 6.7110e-04 - val_loss: 0.0023\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 6.7633e-04 - val_loss: 0.0023\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 6.8524e-04 - val_loss: 0.0022\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 6.6603e-04 - val_loss: 0.0026\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.7578e-04 - val_loss: 0.0023\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 6.8986e-04 - val_loss: 0.0024\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.5115e-04 - val_loss: 0.0025\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 6.7593e-04 - val_loss: 0.0023\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 6.6480e-04 - val_loss: 0.0022\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 6.7373e-04 - val_loss: 0.0024\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 6.7426e-04 - val_loss: 0.0022\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.7165e-04 - val_loss: 0.0023\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.9100e-04 - val_loss: 0.0023\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 6.5877e-04 - val_loss: 0.0023\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.6589e-04 - val_loss: 0.0024\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 6.5731e-04 - val_loss: 0.0023\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.5928e-04 - val_loss: 0.0024\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.5929e-04 - val_loss: 0.0022\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.7116e-04 - val_loss: 0.0022\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 6.5835e-04 - val_loss: 0.0023\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 6.6765e-04 - val_loss: 0.0026\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model10 = Sequential()\n",
    "model10.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model10.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model10.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "model10.add(Dense(256, kernel_initializer='uniform',activation='linear'))\n",
    "model10.add(Dense(256, kernel_initializer='uniform',activation='linear'))\n",
    "model10.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "model10.compile(optimizer=Adagrad(lr=0.009),loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model10.output,y_train_scaled)\n",
    "listOfVariableTensors = model10.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model10.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "history = model10.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Tudo-Loko_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 1.0163 - val_loss: 1.0628\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 1.0000 - val_loss: 1.0628\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEpFJREFUeJzt3X+wXOdd3/H3B6tOCOBYjmVjJAc5oFBMmJJwx3HKUGgcbNllIv8Rt2IIERm1miZuSn9XKZ1xJ4EZ0hYMGUJaTexGzgC2cVOsIaZGyPaQMpHja5w62K6RcKh9sWuLke1CM4QIvv1jHyWb+6x0V3evdu+V3q+ZO3vOc56z96O9Pz53zzm7SlUhSdKwr5t1AEnS6mM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqbNu1gGW68ILL6zNmzfPOoYkrRkPP/zwn1TVhnHmrtly2Lx5M/Pz87OOIUlrRpL/Pe5cDytJkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySpYzlIkjqWgySps2Q5JLk1yQtJfn9o7IIk+5Mcarfr23iSfDjJ4SSPJnnT0D472vxDSXYMjX9vks+3fT6cJCv9j5QknZpxnjl8HNi6aGw3cKCqtgAH2jrAtcCW9rEL+CgMygS4CXgzcAVw0/FCaXN2De23+HNJkqZsyXKoqt8Bji4a3gbsbct7geuHxm+rgYPA+UkuAa4B9lfV0ap6EdgPbG3bzquqz1RVAbcN3ZckaUaWe87h4qp6DqDdXtTGNwLPDM1baGMnG18YMT5Skl1J5pPMHzlyZJnRJUlLWekT0qPOF9Qyxkeqqj1VNVdVcxs2bFhmREnSUpZbDs+3Q0K02xfa+AJw6dC8TcCzS4xvGjEuSZqh5ZbDPuD4FUc7gLuHxt/Vrlq6Eni5HXa6F7g6yfp2Ivpq4N627U+TXNmuUnrX0H1JkmZk3VITkvwq8IPAhUkWGFx19DPAnUl2Ak8DN7Tp9wDXAYeBLwLvBqiqo0k+CDzU5n2gqo6f5H4Pgyuivh74zfYhSZqhDC4SWnvm5uZqfn5+1jEkac1I8nBVzY0z11dIS5I6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6E5VDkn+a5LEkv5/kV5O8MsllSR5McijJHUnObXNf0dYPt+2bh+7n/W38ySTXTPZPkiRNatnlkGQj8I+Buap6A3AOsB34EHBzVW0BXgR2tl12Ai9W1bcDN7d5JLm87fddwFbgl5Kcs9xckqTJTXpYaR3w9UnWAa8CngPeCtzVtu8Frm/L29o6bftVSdLGb6+qL1XVF4DDwBUT5pIkTWDZ5VBVfwz8R+BpBqXwMvAw8FJVHWvTFoCNbXkj8Ezb91ib/5rh8RH7SJJmYJLDSusZ/NV/GfAtwDcA146YWsd3OcG2E42P+py7kswnmT9y5Miph5YkjWWSw0pvA75QVUeq6svAJ4G/CZzfDjMBbAKebcsLwKUAbfurgaPD4yP2+RpVtaeq5qpqbsOGDRNElySdzCTl8DRwZZJXtXMHVwGPA/cD72hzdgB3t+V9bZ22/b6qqja+vV3NdBmwBfjsBLkkSRNat/SU0arqwSR3Ab8HHAMeAfYAnwJuT/JTbeyWtsstwCeSHGbwjGF7u5/HktzJoFiOATdW1V8uN5ckaXIZ/PG+9szNzdX8/PysY0jSmpHk4aqaG2eur5CWJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSx3KQJHUsB0lSZ6JySHJ+kruS/K8kTyR5S5ILkuxPcqjdrm9zk+TDSQ4neTTJm4buZ0ebfyjJjkn/UZKkyUz6zOEXgP9eVX8d+BvAE8Bu4EBVbQEOtHWAa4Et7WMX8FGAJBcANwFvBq4AbjpeKNKkNu/+1KwjSGvSssshyXnA3wJuAaiqv6iql4BtwN42bS9wfVveBtxWAweB85NcAlwD7K+qo1X1IrAf2LrcXJKkyU3yzOF1wBHgvyR5JMnHknwDcHFVPQfQbi9q8zcCzwztv9DGTjTeSbIryXyS+SNHjkwQXZJ0MpOUwzrgTcBHq+qNwP/jq4eQRsmIsTrJeD9Ytaeq5qpqbsOGDaeaV5I0pknKYQFYqKoH2/pdDMri+Xa4iHb7wtD8S4f23wQ8e5JxSdKMLLscqur/AM8k+Y42dBXwOLAPOH7F0Q7g7ra8D3hXu2rpSuDldtjpXuDqJOvbieir25gkaUbWTbj/+4BfTnIu8BTwbgaFc2eSncDTwA1t7j3AdcBh4IttLlV1NMkHgYfavA9U1dEJc0mSJjBROVTV54C5EZuuGjG3gBtPcD+3ArdOkkWStHJ8hbQkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqWM5SJI6loMkqTNxOSQ5J8kjSX6jrV+W5MEkh5LckeTcNv6Ktn64bd88dB/vb+NPJrlm0kySpMmsxDOHnwCeGFr/EHBzVW0BXgR2tvGdwItV9e3AzW0eSS4HtgPfBWwFfinJOSuQS5K0TBOVQ5JNwN8BPtbWA7wVuKtN2Qtc35a3tXXa9qva/G3A7VX1par6AnAYuGKSXJKkyUz6zOHngX8F/FVbfw3wUlUda+sLwMa2vBF4BqBtf7nN/8r4iH0kSTOw7HJI8sPAC1X18PDwiKm1xLaT7bP4c+5KMp9k/siRI6eUV5I0vkmeOXwf8PYkfwTczuBw0s8D5ydZ1+ZsAp5tywvApQBt+6uBo8PjI/b5GlW1p6rmqmpuw4YNE0SXJJ3Mssuhqt5fVZuqajODE8r3VdWPAvcD72jTdgB3t+V9bZ22/b6qqja+vV3NdBmwBfjscnNJkia3bukpp+xfA7cn+SngEeCWNn4L8Ikkhxk8Y9gOUFWPJbkTeBw4BtxYVX95GnJJksa0IuVQVQ8AD7TlpxhxtVFV/Tlwwwn2/2ngp1ciiyRpcr5CWpLUsRwkSR3LQZLUsRwkSR3LQZLUsRwkSR3LQZLUsRwkSR3LQWekhd2fnnUEaU2zHCRJHctBktSxHCRJHctBktSxHCRJHctBktSxHCRJHctBktSxHCRJHctBktSxHCRJHctBktSxHCRJHctBktSxHCRJHctBktSxHCRJHctBktSxHCRJnWWXQ5JLk9yf5IkkjyX5iTZ+QZL9SQ612/VtPEk+nORwkkeTvGnovna0+YeS7Jj8nyVJmsQkzxyOAf+8qr4TuBK4McnlwG7gQFVtAQ60dYBrgS3tYxfwURiUCXAT8GbgCuCm44UiHbd596dmHUE6qyy7HKrquar6vbb8p8ATwEZgG7C3TdsLXN+WtwG31cBB4PwklwDXAPur6mhVvQjsB7YuN5ckaXIrcs4hyWbgjcCDwMVV9RwMCgS4qE3bCDwztNtCGzvR+KjPsyvJfJL5I0eOrER0SdIIE5dDkm8E/ivwT6rq/55s6oixOsl4P1i1p6rmqmpuw4YNpx5WkjSWicohyV9jUAy/XFWfbMPPt8NFtNsX2vgCcOnQ7puAZ08yLkmakUmuVgpwC/BEVf3c0KZ9wPErjnYAdw+Nv6tdtXQl8HI77HQvcHWS9e1E9NVtTJI0I+sm2Pf7gB8DPp/kc23s3wA/A9yZZCfwNHBD23YPcB1wGPgi8G6Aqjqa5IPAQ23eB6rq6AS5JEkTWnY5VNX/YPT5AoCrRswv4MYT3NetwK3LzSJJWlm+QlqS1LEcJEkdy0GS1LEcJEkdy0Eawfdy0tnOcpAkdSwHSVLHcpAkdSwHSVLHcpAkdSwHSVLHcpAkdSwHSVLHcpBWkC+e05nCctAZ4SP/8L5ZR5DOKJaDJKljOUiSOpaDThuPv0trl+UgSepYDlqTvnvvd5/S/AP3fdtpSrIyfJal1cZy0FnlVEtFOltZDpKkjuUgSepYDlrSmXw83BfPSaNZDpKkjuWgNWW1X3W0GpzJz/Q0PZbDWWQ1/tJYfPXQ4sM8C7s/Pc04J/x8XuWks82qKYckW5M8meRwkt2zziNJZ7NVUQ5JzgE+AlwLXA78SJLLZ5tKp9NaPzw0zonshd2f9hmH1qxVUQ7AFcDhqnqqqv4CuB3YNuNMOoHhX3gn+iU/68NDp9Nyi+10P1Zr7TFejYc59VWrpRw2As8MrS+0salY/EO13B/iae13/Ifq+H7LPW6/+Jf8uH8Na7rG/Tovd79Jv68W/5Jfie+rlS7g0/X5TtVaKsRU1awzkOQG4Jqq+vtt/ceAK6rqfYvm7QJ2tdXvAJ5c4q4vBP5kheOeDmslJ6ydrGslJ6ydrGslJ5j1RL61qjaMM3Hd6U4ypgXg0qH1TcCziydV1R5gz7h3mmS+quYmj3d6rZWcsHayrpWcsHayrpWcYNaVsFoOKz0EbElyWZJzge3AvhlnkqSz1qp45lBVx5L8I+Be4Bzg1qp6bMaxJOmstSrKAaCq7gHuWeG7HfsQ1IytlZywdrKulZywdrKulZxg1omtihPSkqTVZbWcc5AkrSJnVDkkuSDJ/iSH2u36E8x7bZLfSvJEkseTbF6NOdvc85L8cZJfnGbGoc+/ZNYk35PkM0keS/Jokr83xXwnfduVJK9Ickfb/uC0v9aLsiyV9Z+178dHkxxI8q2rMefQvHckqSQzu9JmnKxJ/m57XB9L8ivTztgyLPW1f22S+5M80r7+180i59eoqjPmA/j3wO62vBv40AnmPQD8UFv+RuBVqzFn2/4LwK8Av7haH1Pg9cCWtvwtwHPA+VPIdg7wh8DrgHOB/wlcvmjOe4H/1Ja3A3fM6HEcJ+vfPv69CLxnFlnHydnmfRPwO8BBYG4VP6ZbgEeA9W39olWacw/wnrZ8OfBHs3hMhz/OqGcODN5yY29b3gtcv3hCe8+mdVW1H6Cq/qyqvji9iMAYOQGSfC9wMfBbU8o1ypJZq+oPqupQW34WeAEY64U2ExrnbVeG898FXJUkU8i22JJZq+r+oe/Fgwxe7zNt476VzQcZ/OHw59MMt8g4Wf8B8JGqehGgql6YckYYL2cB57XlVzPidV7TdqaVw8VV9RxAu71oxJzXAy8l+WR7Cvcf2hv/TdOSOZN8HfCzwL+ccrbFxnlMvyLJFQz+OvrDKWQb521XvjKnqo4BLwOvmUK2xU71LWJ2Ar95WhONtmTOJG8ELq2q35hmsBHGeUxfD7w+ye8mOZhk69TSfdU4Of8d8M4kCwyu2nwfM7ZqLmUdV5LfBr55xKafHPMu1gHfD7wReBq4A/hx4JaVyHfcCuR8L3BPVT1zuv/QXYGsx+/nEuATwI6q+quVyLbUpxwxtvjyu3HmTMPYOZK8E5gDfuC0JhrtpDnbHy03M/iZmbVxHtN1DA4t/SCDZ2KfTvKGqnrpNGcbNk7OHwE+XlU/m+QtwCdazmn8HI205sqhqt52om1Jnk9ySVU9135RjXoKuQA8UlVPtX1+HbiSFS6HFcj5FuD7k7yXwXmRc5P8WVWt+P91sQJZSXIe8Cng31bVwZXOeALjvO3K8TkLSdYxeMp+dDrxRuY4buRbxCR5G4NS/oGq+tKUsg1bKuc3AW8AHmh/tHwzsC/J26tqfmopB8b9+h+sqi8DX0jyJIOyeGg6Eb+SYamcO4GtAFX1mSSvZPCeS7M4DAaceYeV9gE72vIO4O4Rcx4C1ic5fkz8rcDjU8g2bMmcVfWjVfXaqtoM/AvgttNRDGNYMmt7y5P/xiDjr00x2zhvuzKc/x3AfdXO+k3Zklnb4Zr/DLx9RsfGYYmcVfVyVV1YVZvb9+ZBBnmnXQxLZm1+ncGJfpJcyOAw01NTTTlezqeBqwCSfCfwSuDIVFMuNusz4iv5weBY8gHgULu9oI3PAR8bmvdDwKPA54GPA+euxpxD83+c2V2ttGRW4J3Al4HPDX18z5TyXQf8AYNzHD/Zxj7A4BcWDH7Ifg04DHwWeN0Mvz+XyvrbwPNDj+G+1Zhz0dwHmNHVSmM+pgF+jsEfgJ8Htq/SnJcDv8vgSqbPAVfP6jE9/uErpCVJnTPtsJIkaQVYDpKkjuUgSepYDpKkjuUgSepYDpKkjuUgSepYDpKkzv8HyDfrYuDYIfIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model11 = Sequential()\n",
    "model11.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='softplus'))\n",
    "model11.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "model11.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "model11.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "model11.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "model11.add(Dense(1, kernel_initializer='he_uniform',activation='softplus'))\n",
    "model11.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model11.output,y_train_scaled)\n",
    "listOfVariableTensors = model11.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model11.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "history = model11.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "\n",
    "pyplot.hist(evaluated_gradients, bins='auto')\n",
    "pyplot.savefig(os.path.join(my_path, 'Hist_Soft_Trei_he_%s.png')%hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 446us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 4s 421us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 4s 423us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 4s 417us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 4s 418us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 4s 404us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 4s 404us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 4s 405us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 4s 404us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 4s 404us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: nan - val_loss: nan\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 4s 401us/step - loss: nan - val_loss: nan\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: nan - val_loss: nan\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: nan - val_loss: nan\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: nan - val_loss: nan\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 4s 405us/step - loss: nan - val_loss: nan\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 4s 403us/step - loss: nan - val_loss: nan\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: nan - val_loss: nan\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: nan - val_loss: nan\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: nan - val_loss: nan\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: nan - val_loss: nan\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: nan - val_loss: nan\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: nan - val_loss: nan\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: nan - val_loss: nan\n",
      "Epoch 244/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 4s 405us/step - loss: nan - val_loss: nan\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 4s 405us/step - loss: nan - val_loss: nan\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: nan - val_loss: nan\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: nan - val_loss: nan\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: nan - val_loss: nan\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f01da66a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model12 = Sequential()\n",
    "model12.add(Dense(1024, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "model12.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "model12.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model12.output,y_train_scaled)\n",
    "listOfVariableTensors = model12.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors)\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model12.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]\n",
    "\n",
    "model12.fit(X_train_scaled.values, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled.values, y_val_scaled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
